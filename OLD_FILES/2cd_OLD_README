# SPX
### SuperPositioning Text & Data to a Datagraph (LIVE)
<hr></br></br>
&nbsp; &nbsp; &nbsp; To start off, this entire project began as I (3Douglas) was trying to come up with a determinable compression method that would mimic SHA hashes. I built a model to handle how inputs would be handled and it came down to wrapping data around a pole as if it was on a grid but I needed a way to determine "turning right" and "turning left" so I could "ensure wrapping goes the intended direction regardless of object-shape". I turned to the Mota Club for help. Jake La Doge came to my hand by offering guidance and accidentally found out about "the Futurama Theroem" as a mathematical way to handle "turning".</br>&nbsp; &nbsp; &nbsp; Using the Futurama Theroem didn't correct things like we wanted but is used to correct shifted data so we can push data in and out of "phase" so to speak. There after, this has been mostly modge-podged together. Much of the wording in the original whitepaper & GIT Readme were based on what we were later doing but the original goal was to "entangle Input to APIs" or at the least use this to "Compound Hamming Code Graphs" to reduce online datablob sizes.
</br></br>&nbsp; &nbsp; &nbsp; Before starting the actual Read_ME for SuPosXt (SPX), let's get you up to date. Almsot all of the original processes in the original SPX system has been reduced to just a lookup table. We will get into more of the upgrades after this but if you wish to read the Original README file, <a rel='noopener;noreferrer' href="https://github.com/DigiMancer3D/SPX/blob/main/OLD_FILES/OLD_README" style="link-decoration:none;color:limegreen;" title="   OLD_FILES/README" target="_blank">click here</a>.
</br></br></br>
<h1 style='align:center;text-align:center;'><b>Spherical-Parallelogram Alignment</b> <sub><sup>[<sub><i>VERIFYING&nbsp;CONCEPT</i></sub>]</sup></sub></h1>
</br></br>
&nbsp; &nbsp; &nbsp; As we build more dynamic ways to finding & correcting errors, we often need verfiying points of information to help ensure we don't <i>Over Process</i> data or even <i>Correct False Negatives</i>. The idea is to have a way to verify lengths without knowing anything more then the length or derrivative totals of stored data-points during processing.</br>&nbsp; &nbsp; &nbsp; Imagine you have a sphere better yet, imagine a basketball or futball. Similar shapes BUT different patterns. Okay so we want to take a flat sphered-ball and cut grooves into it, specifically a parallelogram but we want the parallelogram to touch or intersect but not overlap or simply not be touching. To do this we need a few measurements.<ul><li>Sides A & C of the Parallelogram</li><li>Sides B & D of the Parallelogram</li><li>"a", "b", Height, "P" & Area of the Parallelogram</li><li>Volume of the Sphere</li><li>Radious of the Sphere</li><li>a <q><i><b>Magic Number</b></i></q></li><li>Binary Represented Bit</li><li>Input Standing</li></ul></br></br>&nbsp; &nbsp; &nbsp; We want to use some of the first set of variables to find others like the Area of a Parallelogram & the "<b><i>Magic Number</i></b>". Next will just be the maths used for each.<ul><li>z = decimal-output-of-input</li><li>y = Current_micro-loop_position</li><li>x = Number_of_currently_seen_inputs</li><li>[Angle] a = <b>(((y * 18) + x) - 1)</b></li></li></ul></br><ul><li>A|C = <b>Round((trunc[z * Pi]) + ((a) % 2)^((Pi + z) / (a)))</b></li><li>B|D = <b>Round((180-(trunc[z * Pi]) + (((a) % 2) ^ ((Pi + z) / (a)))))</b></li><li>[Angle] b = <b>Round(a / sin([round-up{trunc[z * Pi]} + ((a % 2) ^ ((Pi + z)/a))]))</b></li><li>[Height] h = <b>Round(a * (a / sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / a)) * sin([round-up]{(trunc[z & Pi] + a % 2) ^ ((Pi + z) / a)})}))) <sub><sup>if output is negitive, drop the negitive sign</sup></sub></b></li><li>P = <b>Round((a * 2) + (a / sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / (a * 2)))})))</b></li><li>Area = <b>Round((1/2) * (a / (sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / ((a * (a * (a / sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / ((a * (sin([round-up]{a}])))))})))))))}))))</b></li></ul></br><ul><li>Sphere Volume = <b>Round((4/3) * Pi * POW(trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / ( a * (sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / a))}))))){drop any negitive sign at this point} / 2, 3))</b></li><li>Sphere Radius = <b>Round(a * (a / (sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / (a * (sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / (a)))})))))})))){drop any negitive sign at this point} / 2</b></li></ul></br><ul><li><q><i>Magic Number</i></q> = <b>Round(Round(Round(((4/3) * Pi) * POW(Round(a * (a / sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / (a * (sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / (a)))})))))}))){drop negitive sign at this point} / 2, 3)) % ((1/2) * (a / (sin[round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / (a * (a * a)) / (sin([round-up]{trunc[z * Pi]  + ((a % 2) ^ ((Pi + z) / a))}))))})))) / Round(a * (a / (sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi +z) / (a * (sin([round-up]{trunc[z * Pi] + ((a % 2) ^ ((Pi + z) / a))})))))}))){drop any negitive sign at this point} / 2))</b></li></ul></br></br>&nbsp; &nbsp; &nbsp; If everything is correct, all numbers should be positive and the Magic Number should be greater than zero for as long as a, h, Area or Sphere Volume is 1 or greater. More importantly if all the maths is correct we should be able to take a sphere and determine the size the parallagram needs to be to wrap compeltely around the sphere to have only 1 line the same width of the other cut lines that joins the top and bottom edges of the parrallgram. Engraving a parrallagram into a ball would end up with a top line and bottom line (representing the top and bottom edge respectfully), but if the maths is wrong the top/bottom line connector (which is angled) will either be too fat or there will be two. We want all the lines engraved in the ball to be the same thickness, width & depth.
</br></br>
<h1 style='align:center;text-align:center;'><b>Full House Talk Model</b> <sub><sup>[<sub><i>CONCEPT</i></sub>]</sup></sub></h1>
</br></br>
&nbsp; &nbsp; &nbsp; In the classic American TV sitcom, "<b>Full House</b>" (<a href='https://www.imdb.com/title/tt0092359/' target='_blank' rel='noopener noreferrer'><b>TV series 1987-1995, IMDB</b></a>) when the Tanner family has a visit from their eldest daughter's firend, Kimmy Gibbler, she would often speak about things that only DJ understands. Stephanie, DJ's younger sister, can explain what DJ transcribes to her dad but not accurately, so Michelle (the youngest Tanner child) would be needed to fill in the gaps or at least get the order correct. In the end, the father and his cohort of male-friend-guardians are often still left unusre of what is being said, so they need the help of Becky to decode what the men have to make sense of the situation. In the show this made a great dynamic that showed how differently generations speak because of the influences of their own sub-cultures of school, family, situations of life & public media.</br>
&nbsp; &nbsp; &nbsp; The <i>Full House Talk Model</i> uses the idea of bait & switch with the information to make what's actually being said unclear. In theory, we have groups or subcultures of inputs that will allow us to have some form of control during the decoding process. So if Michelle is only remembering a moderate number of numerical patterns, say like, "770, 55.01, 111; 6" then we can make the data seem almost the same unless you know what the patterns mean to each family. So if Michelle talks about something outside her subculture, she might get the words wrong but the general meaning is there and from there we can determine the actual intended information. Same thing, our index is only keeping track of the family of input seen while our graph-rows (also called data-rails) tracks the binary patterns that represent the input seen. While most of this is pre-determined as a look-up table, we can still do some amount of potential error correcting ahead of time as well as use the same "error correcting" method to force an alignment order mechenism for our algo to align to.</br>&nbsp; &nbsp; &nbsp; The <b>Futurama Theorem</b> will help keep everything aligned and settled with modular sections in the talk model & modified versions of the theorem will be used to find-&-fix errors as well as turn anything into a single binary representation for new parity checks. Hopefully when we are done, we'll be able to pull the full conversation from very little information just like they do on the show.</br>
&nbsp; &nbsp; &nbsp; The Full House Talk Model's base for encoding is to superposition the input to a font designed to be "visually encrypted" called <b><i>Node</i></b>. This means the the data we are switching the user inputted data for an end-2-end encrypting font represented Maths string. We use the Maths string to determine when moving up and down in the graph to drop the bits but also using a rule of thumb, "2-left then anything over 1 has to go up". So if we have anything in the middle of that graph's row:"xx!0", then you have to move to a new row. 
</br></br>
<h1 style='align:center;text-align:center;'><b>Determinable Dynamic Token-Swap Compression </b> <sub><sup>[<sub><i>COMPRESSION</i></sub>]</sup></sub></h1>
</br></br>
&nbsp; &nbsp; &nbsp; We give the system 13 of the smallest patterns and we have the system then build up the rest of the patterns that'll be used. Because we plan on eventually allow users to change these pattern points we want the dynamic pattern generation to have some gaurd-rails and most of that is strickly in the limiting process per data-array. Utilizing the american alphabet via case-letter, we get double the use. We will be using the captial case-letters to showcase "number of times seen in a row" or the 'in-row number'. We have 3 major compression points: Rows, Michelle Index (sometimes called Steph-codes), Dividing Data. Because of this, we have divided the lower-case alphabet into 4 sections. First the lower-case a (also paired with the uppercase A), then the lower-case characters "b" through "n", next "o" through "s", then finally "t" through "z". This allows us to have two major pattern storage groups for this dynamic pattern generation process.</br></br>
&nbsp; &nbsp; &nbsp; We are going to first map the 13 given patterns to "b-n": "0", "1", "001101", "01011", "10100", "0110", "1001", "010", "101", "011", "100", "10", "01", "11", "00". The order is very imporant, we want the largest possible token to be used first in every data-array & as we generate patterns, they get bigger so we will use the generated data-arrays in reverse-made order, so newest first but still "0", "1", then biggest to smallest. The order or pattern we give the this initial pattern will determine how the dynamic generation process will go. As we go through the process of generation, we will check for duplicates and prevent duplicates in various data-arrays for the highest possible number of swappable tokens. We will build token to this order specification: "as given (aka 'a-g')", "a-g + reversed (aka  palindrome or 'pal')", "expandable reversed a-g (aka reversed a-g && reversed a-g + reversed a-g)", "reversed a-g + a-g + a-g + reversed a-g", "reversed a-g + reversed a-g + a-g + a-g", "1st Generated Pattern (1GP) + reversed 1GP", "in-row (up to row length & dynamically decresed) 1, 0, the given patterns", "dual a-g in-row patterns (also up to row length  & dynamically decresed)". The order of operations is the specification to help make this determinable, along side with the given patterns.
</br></br>
<h1 style='align:center;text-align:center;'><b>Transmuting & Checking </b> <sub><sup>[<sub><i>ALGO</i></sub>]</sup></sub></h1>
</br></br>
&nbsp; &nbsp; &nbsp; Once we have the array of the 4 rows of the superpositioning graph, we can change what binary-strings we just had it create. Starting at the null-position of all the rows, record the first object seen. Once done, move to check each bit in the row recording if the same as before (1) or different as before (0). This means the sequence "1001" would be recorded as "1010". Each row is treated individually to the other rows, but all four rows are checked per loop cycle.
</br>
&nbsp; &nbsp; &nbsp; With our setup, we are cheating by having a loop order around the input. So when we are checking the parity system we are using a maths check so we have to keep the input per check under 19 characters (I believe this is a JS limitation or the browser pointer limitation). So I have come up with the loop-order cheat. Because I don't have to keep track of what is actually being tested directly, I only need to test the previous to the next, I can take the parity check of any 18-character string and use it as the first bit in the next check (which would be 17-character string Plus the original parity bit). Do this for each check and we can always have an accurate check every 18 inputs. This loop order did make complications because this means we are always in a loop and will always have the data fully baked in until we want to "save" a graph. This does allow us to somewhat "auto-compress" some graphed outputs but not all will be like this. This should still leave an opening for attaching multiple of these into a compressed data graph as the sub-original demands (being able to nest these together).</br></br>
&nbsp; &nbsp; &nbsp; This system allows for a multitude of ways to do parity checks so we will go over the some possible parity checks. A parity check is determining if data was changed. Parity checks in the manner we are looking at are very common in (7,4) Hamming Code but for what we are doing, we are not checking to see if the binary of the state change rows were changed. This will tell the decoder how to tell if the binary of the data has any possible errors and possible corrections. Because we are not directly working with only binary, we can use the determiner slots to drop almost any data ie: the total-byte size of the end datagraph, encode hard-error-detection markers, apply trinary computational actions (the magic number for example), entangle data identifiers (like the derrivate parralagram area minus the sphere area), entangle API markers or nearly anything else with Data-Wave H-APIs.</br>
&nbsp; &nbsp; &nbsp; The most common way to parity check is the (7,4) Hamming code method of checking specific row associations to cross-check the entirety of the datagraph. (7,4) Hamming code looks at 2 rows/columns at a time per parity check slot. 1st parity slot checks columns 2 & 4, place a 0 or 1 in D1 to ensure this check has an even number of ones. 2cd parity slot checks columns 3 & 4, place a 0 or 1 in D2 to ensure this check has an even number of ones. 3rd parity slot checks rows 2 & 4, place a 0 or 1 in D3 to ensure this check has an even number of ones. 4th parity slot checks rows 3 & 4, place a 0 or 1 in D4 to ensure this check has an even number of ones. Using trinary actions would allow you to use also a "2" in each D-slot to say that at that point, with the "2" being two "1's" as well. If there is an even number of the ones in the entire graph (each determinor point would be a '2', there are a total of four determinor spots).</br>
&nbsp; &nbsp; &nbsp; The alternative parity check method is the (11,1) Determinless method that is using the D-slots for any value with bases higher than 3 (integers over "2", decimals, binary, Trinary, or any numerical-digit-representation of non-numerical objects) but does the parity checks by running any non-binary data through a Modified Futurama Theorem (<i>MFT</i>) <b>[Ceiling|^Trunc_(Trunc|_(n*3.14)_|+z)_|%2^|]</b> and using the <i>MFT</i> output for the parity check. This method can still read parity checks in the method of row association checking number of ones to create dynamically-modular or ruggedly-singular checks which may include but are not limited to the following: Find Errors, Correct Errors, Mutate Data, Layer Data, Lattice-fill (quantum resistance), fast-verifications.</br>
&nbsp; &nbsp; &nbsp; It's the Modified Futurama Theorem that allows for dynamic determining slots because it turns any decimal or binary representation of what we give it into a single binary bit. Placing the data as a whole input for 'n' as it's decimal form and it's length as z we can help ensure we reliably get the same output for the same inputs. Recording the result of the parity check goes into the null position at the end as a non-numerical character that ensures the number of ones for the parity check type has an equal number of ones. Every time this recording needs to happen after the inital time, we store the new 0 or 1 parity check bit as a variable instead of changing the actual recorded bit. Changing the variabled-bit with each loop in the pattern gives us a "Meta-Pattern" that can be used to check for potenitally falsified data. Once the parity sequence is done the ending variable parity bit should be the same as the initial recorded parity bit. So, re-perform the inital recorded parity check to ensure the inital needed bit is the same as the final recorded bit, as seen following: A+B=1;C+D=x;E+F=y;G+H=z;A+B+[...}+H=(A+B). By having this "Unknown Meta-Pattern" and because the MFT we are using is generically using the infinate-average of binary 1's from what we give it, we can ensure we have the correct corresponding data. The infinate-average is a generic way of saying, "the average of everthing becomes the ending result" so if the MFT looks at the total number of 1's in the binary form of what we give it, then as we give it more data we are more likely to end at 0 so one check being of the sum of all the other checks has a more-likely chance of being 0 then the inintal first check (which is included in the sum of all other checks check). So if both is 0, we can say it's possibly fomred but if both are 1, we can say it's imporabably formed (not-likely to be formed) and thustly if both do not align it's possibly errored.</br>
&nbsp; &nbsp; &nbsp; The wrapper is just about ready. We use an ABI (Aplication Binary Interface) to display what type of data the wrapper application process needs. If you don't have the data for a particular section, just send any non-numerical and non-speciality-characters, like a alphabet character. Now we mix in the final values of our header ABI, which is as follows:</br>
<code><ul>datagraph {
<ul>Graphhash [
<ul>avg loop cycle (numerical; decimal preferred);
"x";
internal data size (numerical; decimal preferred);
"e" (normal encoding) |or| "f" (compressed encoding) |or| "g" (graphed encoding);
chain weight (numerical; decimal preferred);
nonsensitical non-numeric parity bits (Alphabetical; obscure latin-characters preferred)</ul>
 ],
Determiner slot 1 [
<ul>anything</ul>
 ],
Determiner slot 2 [
<ul>anything</ul>
 ],
Programming slot 1 [
<ul>anything</ul>
 ],
Determiner slot 3 [
<ul>anything</ul>
 ],
Row 1 [
<ul>anything;
anything;
anything</ul>
 ],
Determiner slot 4 [
<ul>anything</ul>
 ],
Row 2 [
<ul>anything;
anything;
anything</ul>
 ],
Programming slot 2 [
<ul>anything</ul>
 ],
Row 3 [
<ul>anything;
anything;
anything
<ul>"."
nullstop
<ul>Row 0 [
<ul>anything
 </ul>] </ul>], </ul>}</ul></ul></ul></code></br></br>
&nbsp; &nbsp; &nbsp; Once the wrapper is finished we can send that graph and set of subgraphs. These graphs and subgraphs are designed around keeping the data intact within the specified order to ensure the data is reversable even if it's split into sub-sections or sub-graphed.</br>
&nbsp; &nbsp; &nbsp; Using the Modified Futurama Theorem to turn any data into a parity check does open up more potential of oppritunities but in the same way that modified maths formula gives us provability without prior knowledge. These are Minimal-Knowledge Proofs for smart storage & smart launching data-enriched applications.</br>
</br>
<h1 style='align:center;text-align:center;'><b>Handling Complexities of Detangling</b></h1>
</br></br>
&nbsp; &nbsp; &nbsp; Entangling data is easy, it's a simple one-direction mathematical formula but being able to detangle requires a bit more thought and planning. We encode to entangle in a very specific way so we can detangle in a very generic way. This allows for direct computational entangling so everyone can have unique entangles but the way we detangle them will run all the same rules. Instructions within a rule can be swapped and changed to have an additional layer of decoding obfuscation. The first "rule-of-thumb" so-to-speak, We use a 17-Grid (4x4 Grid [16] + Null-Grid Slot [17th]) to keep track of the shapes the complexity will form. This is the most important rule, this allows us to take an unknown direction and formulate it into a relative direction that we can reverse. We spiral the data, always to the right, around the centered "<i>null-slot</i>" which keeps the shapes all aligned together. Now we have a way to form order, let's go over the rules of concept for "<i>Node-Based</i>" en/detangling.
</br></br>
<table>
<tr><td></td><td></td><td>17-Grid</td><td></td><td>(visual)</td><td></td></tr>
<tr><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Row 1: </td><td>1</td><td>10</td><td></td><td>100</td><td>1000</td></tr>
<tr><td>Row 2: </td><td>2</td><td>20</td><td></td><td>200</td><td>2000</td></tr>
<tr><td>Null: </td><td>-</td><td>--</td><td>0</td><td>---</td><td>-- --</td></tr>
<tr><td>Row 3: </td><td>3</td><td>30</td><td></td><td>300</td><td>3000</td></tr>
<tr><td>Row 4: </td><td>4</td><td>40</td><td></td><td>400</td><td>4000</td></tr>
</table>
</br></br>
To write out a 17-grid you use this formation:<ul>Row1-Slot1,R1S2,R1S3,R1S4;Row2-Slot1,R2S2,R2S3,R2S4;Row3-Slot1,R3S2,R3S3,R3S4;Row4-Slot1,R4S2,R4S3,R4S4;NULL-SLOT.</ul></br>When doing the maths behind this grid try: <ul><ul><b>x*y <i>=== row*cols</i></b> but when adding use this method: <ul><b>x <i>=== Row Position (x1,x2,x3,x4,[...],x[n])</i></b></ul><ul><b>y <i>=== Column Position (y1,y2,y3,y4,[...],y[n])</i></b></ul><ul>&nbsp;</ul>This works for grids greater than 17 as long as there's a null slot dead-center in the graph, that forcefully offsets some objects by individual complexities so where the more complex the object the higher chance it'll auto-reset to the null-slot, helping us force order without prior knowledge.</ul>
</br></br>
<h2 style='align:center;text-align:center;'><b>17-Grid Rules</b></h2>
</br></br>
<code>
<ul>
<ol>Use Binary-Grid identifiers for data-mapping <i>(<b>0,1,10,11,100,101,110,[...]1110,1111,10000</b>)</i></ol>
<ol>Use Forward-Down identifiers for complexity-mapping <i>(<b>1,10,100,1000,2,20,200,[...],400,4000,0</b>)</i></ol>
<ol>Use sub-string identifiers for simplistic complexities:<ul><b>+</b> - <i>Either on left of Null or inside Null (use Null-complexity identifiers for determining further)</i></ul><ul><b>-</b> - <i>Right of Null or no Null present at this spot (use Null-complexity identifiers for determining further)</i></ul><ul><b>/</b> - <i>Complex Null-Block (Null-complexity Identifier, Orientation 1)<ul> <b>n/1</b> || <b>0/1</b> (top [<b>black/white</b>] bottom)</ul><ul> <b>n/0</b> || <b>1/0</b> (top [<b>white/black</b>] bottom)</ul></ul><ul><b>\</b> - <i>Complex Null-Block (Null-complexity Identifier, Orientation 2)<ul> <b>n\1</b> || <b>0\1</b> (bottom [<b>white/black</b>] top)</ul><ul> <b>n\0</b> || <b>1\0</b> (bottom [<b>black/white</b>] top)</ul></ul><ul><b>*</b> - <i>Vertical Multi-Block, always use in Top*Bottom*Lower[*...] formation</i></ul><ul><b>:</b> - <i>Vertical Multi-Block, always use in Top*Bottom*Lower[*...] formation</i></ul></ol>
<ol>Use decimal values as APIs:<ul><b>0</b> - <i>[<b>NO DECIMAL NULL</b>] Space Object (empty)</ul><ul><b>0.01</b> - <i>Center object with objects on both sides</i></ul><ul><b>0.001</b> - <i>Center object value of 0</i></ul><ul><b>0.1</b> - <i>Center object value of 1</i></ul><ul><b>0.2</b> - <i>Center object value of 2</i></ul><ul><b>0.3</b> - <i>Center object value of 3</i></ul><ul><b>0.4</b> - <i>Center object value of 4</i></ul><ul><b>0.5</b> - <i>Center object value of 5</i></ul></ol> &nbsp;Treat each value after "0." in the manner seen, +"01" for center object value of 0, +"1[...]" for center object value of 1 and beyond. This creates a dynamic way to always give extra info on objects during detangling.</ul></br></br>
<ol>Use the known complex objects mapping before venturing into dynamically defined objects:<ol><ul><b>11|22|330|4400</b> - <i>Doubled Same Digits are side-by-side duo x-axis shapes (box-box), trailing zeros are right-shift positions</i></ul><ul><b>3|5|7|30|500|7000</b> - <i>top-down adding of the box-value are duo y-axis boxes, trailing zeros are right shift positions</i></ul><ul><b>111|222|3330|4440</b> - <i>Triple x-axis shape (box-box-box), trailing zeros are right shift positions</i></ul><ul><b>6|90|600|9000</b> - <i>Top-down adding of the box-values are triple y-axis boxes, trailing zeros are right shift positions</i></ul><ul><b>50.1</b> - <i>Unique ID for hard-left trio (duo y-axis shape + Null slot-box), change value behind decimal if needed for new-unique null-slot object value, this is a force centered object</i></ul><ul><b>-500.1</b> - <i>Unique ID for hard-right trio (Null slot-box + duo y-axis shape), change value behind decimal if needed for new-unique null-slot object value, this is a force centered object</i></ul><ul><b>220.1</b> - <i>Unique ID for hard-top trio (duo x-axis shape [box-box] on-top of Null slot-box), change value behind decimal if needed for new-unique null-slot object value, this is a force centered object</i></ul><ul><b>-330.1</b> - <i>Unique ID for hard-bottom trio (Null slot-box on-top of duo x-axis shape [box-box]), change value behind decimal if needed for new-unique null-slot object value, this is a force centered object</i></ul><ul><b>770</b> - <i>Unique ID for duo stacked side-by-side "square" shaped</i></ul><ul><b>55.01</b> - <i>Unique ID for Sandwiched center object with duo stacked shapes (5-sided dice shape), do not change the decimal, this shape should be unique enough to determine front-to-end during detangling</i></ul><ul>Some instructables may be morphed by the encoder/decoder to remove instructable hits or two characters with the same Steph & Michelle Codes. This is done by adding 5 (mathematically) to the left of the "." (or end of code) for example if the algo sees two different hits on -500.1, the second Kimmy code with the same Steph & Michelle code will go from -500.1 to -505.1 without changing the version number nor the extractable instructable. This should be done by the encoder & decoder automatically without question. This is an anti-user-error feature.</ul></ol></ol>
<ol>Trailing Zeros always show right displacement (if 2 on the right-side of grid, 2 zeros should be on the right of left side of decimal for the object code)</ol>
<ol>If reading L2R (left-2-right): Place beside another (ie: 111 or 222)</ol>
<ol>If reading T2B (top-2-bottom): add top-down rows and place L2R beside another (ie: 6 or 9 or 619 or 770)</ol>
</ul>
</code>
</br></br></br>
<code>[check the <a href='https://3dd.in/SPXppr' target='_blank' rel='noreferrer noopener'>Full Tech PvtPpr (private paper)</a> for "Breaking Everything Down" & "Building Everything Back" sections]</code>
</br></br>
<h1 style='align:center;text-align:center;'><b>Finalizing the Point</b> <sub><sup>[<sub><i>CONCLUDE</i></sub>]</sup></sub></h1>
</br></br>
&nbsp; &nbsp; &nbsp; The process does seem to work and can perform the various actions, ie: SuperPositioning text on the fly, recording state changes, Parity Check using MFT formulas, decodable graph output. The idea for at least is a success. With more tweeking & testing more possibilities should be posible later on but at the moment we are able to entangle and detangle some outputs. There are still errors in the works of being fixed and some of the conceptional ideas are still being figured out on how to impliment with Javascript (vanilla).</br>
&nbsp; &nbsp; &nbsp; There are still a few questions left to be answered but all the questions we directly wanted to know were answered. "Do we need to know what we are typing? [no]", "Do we need to send this data? [not now]", "Can something represent data instead [yes]", "Can this be done on the fly? [yes]". But we also made a few more discoveries along the way for example: Can we send near dataless-data? Yes!, Can we send the data at will? Yes!, Can we interlace codes with our near dataless-data? Yes!, Can we decode our encoding & everytime? Yes & not yet everytime, >Can we detect, find & correct multiple errors? Debatable, Can we have single-byte parity checks? YES!, Can we have mathematical only parity checks? YES!.</br>
&nbsp; &nbsp; &nbsp; To end this on a high note, yes my hypothesis seems to be correct or loosely verified under the best of conditions. Encoding does seem to be quick enough, even if with a complex encoding function, to handle on-the-fly encoding for most devices but is potentially strong enough to be used as supplimental data storage or reduant storage. Rather or not the modularity propsed creates more trustlessness is still to be prooven. I honestly do believe this could be a new storage design to loosen what data is being stored and where. Working with the live example and test-beds will get us to build a fully functioning version of this concept as a single webpage. There are the parity check demo and basic encoder demo within the files here on Github but the "testing version" or last-updated of the active version being worked on.</br></br></br>


<sup>Speacial thanks to Jake La`Doge in assiting in the self-correcting methods and directing me to the "Futurama Theorem" which ended up being the glue to getting this concept system to work dynamically.</sup>
</br></br>
<sub>Speacial Thanks to The Mota Club (on telegram) for helping in motivation and spelling corrections for this paper and concept.</sub>
</br></br><hr>
<sub>Note:: There may be spelling errors still.</sub>








